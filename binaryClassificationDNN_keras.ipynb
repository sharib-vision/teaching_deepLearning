{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Deep Neural Network using Keras\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow.\n",
    "\n",
    "\n",
    "1) Cat vs non-cat dataset!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import keras\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare your data\n",
    "# 1(a) load data from .h5 file and convert to numpy array\n",
    "# training set\n",
    "train_dataset = h5py.File('datasets/cat-non-cat/train_catvnoncat.h5', \"r\")\n",
    "train_x = np.array (train_dataset[\"train_set_x\"][:])\n",
    "train_y = np.array( train_dataset[\"train_set_y\"][:])\n",
    "# testing set\n",
    "test_dataset = h5py.File('datasets/cat-non-cat/test_catvnoncat.h5', \"r\")\n",
    "test_x = np.array (test_dataset[\"test_set_x\"][:])\n",
    "test_y = np.array( test_dataset[\"test_set_y\"][:])\n",
    "# class list\n",
    "classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)\n",
    "print(train_x.shape)\n",
    "n_inputs = train_x.shape[1] * train_x.shape[2] * train_x.shape[3]\n",
    "\n",
    "#flatten\n",
    "train_X= train_x.reshape(train_x.shape[0], -1)/255.\n",
    "test_X= test_x.reshape(test_x.shape[0], -1)/255.\n",
    "\n",
    "# warning: check dimensions and be sure of it!!!\n",
    "print('classes are:', classes)\n",
    "print('train_x:', train_X.shape)\n",
    "print('train_y (training labels):', train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building your model\n",
    "model = Sequential()\n",
    "#  recall your layers  (increase it accordingly as above and check what you get)\n",
    "# 20 neurons in 1st hidden layer with input image length and we apply relu\n",
    "model.add(Dense(20, input_dim=n_inputs, activation='relu'))\n",
    "# 7 neurons in 2nd hidden layer with input image length and we apply relu\n",
    "model.add(Dense(7, input_dim=20, activation='relu'))\n",
    "# 5 neurons in 3rd hidden layer with input image length and we apply relu\n",
    "model.add(Dense(5, input_dim=7, activation='relu'))\n",
    "# 1 neuron at the output layer\n",
    "model.add(Dense(1, input_dim=5, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Other optimizers ==> SGD: Stochastic gradient descent optimizer or Adam or RMSprop or Adadelta. Check here (https://keras.io/optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "n_epochs = 150\n",
    "batch_size = 2 # should depend on your data size, here we have very small size data with only 50 test samples and 209 train samples\n",
    "split=0\n",
    "\n",
    "# Case 0: split into 67% for train and 33% for test\n",
    "if split:\n",
    "    train_X, test_X, y_train, test_y = train_test_split(train_X, train_y, test_size=0.33, random_state=1)\n",
    "\n",
    "# Case 1: using your own test data \n",
    "model.fit(train_X, train_y, validation_data=(test_X, test_y), epochs=n_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict from your sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mislabeled_images(classes, X, y, p):\n",
    "    \"\"\"\n",
    "    Plots images where predictions and truth were different.\n",
    "    X -- dataset\n",
    "    y -- true labels\n",
    "    p -- predictions\n",
    "    \"\"\"\n",
    "    a = p + y\n",
    "    mislabeled_indices = np.asarray(np.where(a == 1))\n",
    "    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n",
    "    num_images = len(mislabeled_indices[0])\n",
    "    for i in range(num_images):\n",
    "        index = mislabeled_indices[1][i]\n",
    "        \n",
    "        plt.subplot(2, num_images, i + 1)\n",
    "        plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "        plt.axis('off')\n",
    "        t1 = classes[int(p[index])].decode(\"utf-8\")\n",
    "        t2 = classes[y[0,index]].decode(\"utf-8\")\n",
    "        plt.title(\"Prediction: \" + t1 + \" \\n Class: \" + t2)\n",
    "        \n",
    "    return num_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "index = 10\n",
    "testImage = test_X[index]\n",
    "test_image=testImage.reshape(64,64,3)\n",
    "plt.imshow(test_image, interpolation='nearest')\n",
    "#plt.imshow(X[:,index].reshape(64,64,3), interpolation='nearest')\n",
    "tryImage = testImage.reshape(testImage.shape[0], -1)\n",
    "ynew = model.predict_classes(testImage.reshape(testImage.shape[0], -1).T / 255.0)\n",
    "print(ynew)\n",
    "# show the inputs and predicted outputs\n",
    "print(\"label=%s, Predicted=%s\" % (test_y[index], ynew[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Take a larger dataset\n",
    "\n",
    "2) MNIST dataset\n",
    "\n",
    "This dataset consists of 60,000 training images and 10,000 validation images with digits. \n",
    "\n",
    "The digit classes that you will need to identify will be 0-9 (i.e., 10 classes). This is multi-class classification problem.\n",
    "\n",
    "We will also see how to see our loss is decreasing using tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation and looking into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('datasets/MNIST_data/mnist.npz')\n",
    "x_train= data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Note if this does not work then use below\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "y = 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reshape labels\n",
    "trainY = y_train.reshape(1, y_train.shape[0])\n",
    "testY = y_test.reshape((1, y_test.shape[0]))\n",
    "\n",
    "# look into what images you have\n",
    "# lets have a look at a cat/non cat picture inside our data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "np.random.seed(1)\n",
    "\n",
    "index = 10\n",
    "print(trainY[0][index])\n",
    "\n",
    "plt.imshow(x_train[index])\n",
    "print (\"y = \" + str(trainY[0][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build you model\n",
    "\n",
    "- Make 3 hidden layers with relu activation function\n",
    "- Make 1 output function with softmax \n",
    "```python \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return (e_x / e_x.sum())\n",
    "```\n",
    "- $\\textbf{Softmax}$ function, or normalized exponential function is a generalization of the logistic function that \"squashes\" a K-dimensional vector z \\mathbf {z}  of arbitrary real values to a K-dimensional vector $\\sigma (\\mathbf {z} ) $ of real values, for $K\\geq 2$, where each entry is in the interval (0, 1), and all the entries add up to 1\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run your model\n",
    "\n",
    "You will notice that if you iterate per sample (SGD) then it will be slow. You better process more samples together (batch processing with GD, also called as mini-batch gradient descent). However, depending upon your computational power you will need to adjust your batch size too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize your inputs\n",
    "x_train, x_test = x_train.reshape(x_train.shape[0], -1) / 255.0, x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "n_inputs = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building your model\n",
    "model = Sequential()\n",
    "#  recall your layers  (increase it accordingly as above and check what you get)\n",
    "model.add(Dense(512, input_dim=n_inputs, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation=tf.nn.softmax))\n",
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Compile model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.1086 - val_acc: 0.9832\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0073 - acc: 0.9978 - val_loss: 0.1210 - val_acc: 0.9793\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.1140 - val_acc: 0.9821\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.1220 - val_acc: 0.9816\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.1107 - val_acc: 0.9824\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.1178 - val_acc: 0.9813\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0029 - acc: 0.9989 - val_loss: 0.1173 - val_acc: 0.9816\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1158 - val_acc: 0.9832\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 1.9661e-04 - acc: 1.0000 - val_loss: 0.1171 - val_acc: 0.9838\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.1153 - val_acc: 0.9799\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.1256 - val_acc: 0.9801\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.1120 - val_acc: 0.9822\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0058 - acc: 0.9982 - val_loss: 0.1358 - val_acc: 0.9782\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.1098 - val_acc: 0.9824\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 4.1785e-04 - acc: 0.9999 - val_loss: 0.1137 - val_acc: 0.9836\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0075 - acc: 0.9975 - val_loss: 0.1146 - val_acc: 0.9794\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0028 - acc: 0.9992 - val_loss: 0.1037 - val_acc: 0.9835\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 9.3646e-04 - acc: 0.9997 - val_loss: 0.1155 - val_acc: 0.9820\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.1133 - val_acc: 0.9837\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 1.9807e-04 - acc: 1.0000 - val_loss: 0.1167 - val_acc: 0.9836\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0075 - acc: 0.9982 - val_loss: 0.1132 - val_acc: 0.9791\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.1110 - val_acc: 0.9810\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0013 - acc: 0.9995 - val_loss: 0.1132 - val_acc: 0.9829\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0054 - acc: 0.9982 - val_loss: 0.1083 - val_acc: 0.9826\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1044 - val_acc: 0.9839\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 67us/step - loss: 2.2719e-04 - acc: 1.0000 - val_loss: 0.1072 - val_acc: 0.9842\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 9.6366e-05 - acc: 1.0000 - val_loss: 0.1177 - val_acc: 0.9841\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0107 - acc: 0.9972 - val_loss: 0.0947 - val_acc: 0.9824\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 9.9457e-04 - acc: 0.9998 - val_loss: 0.1004 - val_acc: 0.9836\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.0013 - acc: 0.9996 - val_loss: 0.1177 - val_acc: 0.9817\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 0.0055 - acc: 0.9980 - val_loss: 0.1006 - val_acc: 0.9841\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 4.0771e-04 - acc: 0.9999 - val_loss: 0.1081 - val_acc: 0.9828\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.1197 - val_acc: 0.9815\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.1008 - val_acc: 0.9841\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.1144 - val_acc: 0.9837\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.1102 - val_acc: 0.9829\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 3.8793e-04 - acc: 0.9999 - val_loss: 0.1107 - val_acc: 0.9829\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 4s 61us/step - loss: 5.8221e-04 - acc: 0.9999 - val_loss: 0.1499 - val_acc: 0.9793\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0055 - acc: 0.9983 - val_loss: 0.1292 - val_acc: 0.9801\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.1148 - val_acc: 0.9829\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 4s 58us/step - loss: 0.0014 - acc: 0.9996 - val_loss: 0.1239 - val_acc: 0.9826\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 63us/step - loss: 0.0050 - acc: 0.9983 - val_loss: 0.1123 - val_acc: 0.9820\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.0028 - acc: 0.9991 - val_loss: 0.1203 - val_acc: 0.9827\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.1188 - val_acc: 0.9823\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.0053 - acc: 0.9983 - val_loss: 0.1264 - val_acc: 0.9810\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 0.0030 - acc: 0.9991 - val_loss: 0.1319 - val_acc: 0.9803\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.0012 - acc: 0.9996 - val_loss: 0.1198 - val_acc: 0.9837\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.0016 - acc: 0.9995 - val_loss: 0.1291 - val_acc: 0.9820\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 6s 97us/step - loss: 0.0045 - acc: 0.9985 - val_loss: 0.1193 - val_acc: 0.9823\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 6s 93us/step - loss: 0.0020 - acc: 0.9994 - val_loss: 0.1271 - val_acc: 0.9817\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb23696908>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "# check your loss and accuracy using Tensorboard (see instructions below, if not sure ask your instructor)\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "\n",
    "n_epochs = 50\n",
    "batch_size = 128 \n",
    "# change this and see for yourself\n",
    "'''\n",
    "saves the model weights after each epoch if the validation loss decreased\n",
    "checkpointer = ModelCheckpoint(filepath='tmp/weights.hdf5', verbose=1, save_best_only=True)\n",
    "'''\n",
    "# look into your loss\n",
    "tensorboard = TensorBoard(log_dir=\"tmp/{}\".format(time()))\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=n_epochs, batch_size=batch_size,callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See your loss at tensorboard\n",
    "\n",
    "```shell\n",
    "$tensorboard --logdir logs\n",
    "```\n",
    "\n",
    "http://127.0.0.1:6006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try prediction!!! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alitorch",
   "language": "python",
   "name": "alitorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
