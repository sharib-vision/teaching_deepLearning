{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network using TensorFlow \n",
    "\n",
    "Here we will learn to implement the same using tensorflow.\n",
    "\n",
    "- First we will try to see limitation of small data size for label classification\n",
    "\n",
    "- We will then use large dataset and change neurons in each hidden layers to improve accuracy\n",
    "\n",
    "- Lastly, we will introduce drop outs to regularize our network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First run your cat vs non-cat dataset that you have used before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondRUN=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes are: [b'non-cat' b'cat']\n",
      "train_x: (209, 12288)\n",
      "train_y (training labels): (209,)\n"
     ]
    }
   ],
   "source": [
    "# prepare your data\n",
    "# 1(a) load data from .h5 file and convert to numpy array\n",
    "# training set\n",
    "train_dataset = h5py.File('datasets/cat-non-cat/train_catvnoncat.h5', \"r\")\n",
    "train_x = np.array (train_dataset[\"train_set_x\"][:])\n",
    "train_y = np.array( train_dataset[\"train_set_y\"][:])\n",
    "# testing set\n",
    "test_dataset = h5py.File('datasets/cat-non-cat/test_catvnoncat.h5', \"r\")\n",
    "test_x = np.array (test_dataset[\"test_set_x\"][:])\n",
    "test_y = np.array( test_dataset[\"test_set_y\"][:])\n",
    "# class list\n",
    "classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "\n",
    "#flatten\n",
    "train_X= train_x.reshape(train_x.shape[0], -1)/255.\n",
    "test_X= test_x.reshape(test_x.shape[0], -1)/255.\n",
    "\n",
    "# warning: check dimensions and be sure of it!!!\n",
    "print('classes are:', classes)\n",
    "print('train_x:', train_X.shape)\n",
    "print('train_y (training labels):', train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip this in your first run \n",
    "\n",
    "**MNIST** dataset consists of 60,000 training data and 10,000 test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "if secondRUN:\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, train_y),(x_test, test_y) = mnist.load_data()\n",
    "    train_X, test_X = x_train.reshape(x_train.shape[0], -1) / 255.0, x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "    print(train_X.shape)\n",
    "    print(test_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run from here**\n",
    "\n",
    "- Try changing the number of neuronal connections in each hiddden layes\n",
    "- For MNIST dataset (n_output = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# recall your network but now increase number of neurons here!!! (the accuracy should rise up)\n",
    "n_hidden1 = 200    # 512\n",
    "n_hidden2 = 100    # 256\n",
    "n_hidden3 = 50     #  64\n",
    "n_inputs = train_X.shape[1]\n",
    "samples=train_y.shape[0]\n",
    "n_output = len(classes)\n",
    "if secondRUN:\n",
    "    n_output= 10\n",
    "\n",
    "print(samples)\n",
    "print(n_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensorflow placeholder**\n",
    "\n",
    "A placeholder is simply a variable that we will assign data to at a later step. It allows us to create our operations and build our computation graph, without needing the data. In TensorFlow terminology, we then feed data into the graph through these placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(784)])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None,n_inputs), name=\"X\")\n",
    "# y = tf.placeholder(tf.float32, shape=(None), name=\"y\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neuron layer function**\n",
    "\n",
    "This function will help us to design our layers. It will need parameters: {inputs, number of neurons, the activation function, and the name of the layer}.\n",
    "\n",
    "Recall,\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (weighted sum of inputs plus biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_layer (A, n_neurons, name, activation=None):\n",
    "    with tf.name_scope(name):\n",
    "        n_inputs = int(A.get_shape()[1])\n",
    "        stddev = 2/np.sqrt(n_inputs+n_neurons)\n",
    "        \n",
    "        # Outputs random values from a truncated normal distribution. (Note: this will allow faster convergence)... \n",
    "        # you can play with this and see how it affects ...\n",
    "        init = tf.truncated_normal((n_inputs,n_neurons), stddev = stddev)\n",
    "        W = tf.Variable(init, name=\"kernel\")\n",
    "        b = tf.Variable(tf.zeros([n_neurons]), name =\"bias\")\n",
    "        \n",
    "        Z = tf.matmul(A, W)+b\n",
    "        if activation is not None:\n",
    "            return activation (X)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hiddden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "    hiddden2 = neuron_layer(hiddden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "    hiddden3 = neuron_layer(hiddden2, n_hidden3, name=\"hidden3\", activation=tf.nn.relu)\n",
    "    \n",
    "    logits = neuron_layer(hiddden3, n_output, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with drop-out\n",
    "# with tf.name_scope(\"dnn\"):\n",
    "#     hiddden1 = neuron_layer(X, n_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n",
    "#     hiddden2 = neuron_layer(hiddden1, n_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n",
    "#     hiddden3 = neuron_layer(hiddden2, n_hidden3, name=\"hidden3\", activation=tf.nn.relu)\n",
    "# #     hiddden3 = tf.nn.dropout(hiddden3, keep_prob)\n",
    "#     drop_out = tf.nn.dropout(hiddden3, 0.25)  # DROP-OUT here\n",
    "#     logits = neuron_layer(drop_out, n_output, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define loss or compute cost**\n",
    "\n",
    "Here we have used, [sigmoid cross entropy loss](https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    l = tf.reduce_mean(xentropy, name = \"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op=optimizer.minimize(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "save = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\", l)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1.4478667 Training (batch) accuracy: 0.69921875 Val accuracy: 0.6627\n",
      "epoch: 10 loss: 0.6228868 Training (batch) accuracy: 0.82421875 Val accuracy: 0.87\n",
      "epoch: 20 loss: 0.39652473 Training (batch) accuracy: 0.8984375 Val accuracy: 0.8854\n",
      "epoch: 30 loss: 0.39734223 Training (batch) accuracy: 0.8984375 Val accuracy: 0.8933\n",
      "epoch: 40 loss: 0.3940316 Training (batch) accuracy: 0.91796875 Val accuracy: 0.8976\n",
      "epoch: 50 loss: 0.4544785 Training (batch) accuracy: 0.87109375 Val accuracy: 0.9014\n",
      "epoch: 60 loss: 0.42724705 Training (batch) accuracy: 0.8828125 Val accuracy: 0.9046\n",
      "epoch: 70 loss: 0.4051661 Training (batch) accuracy: 0.890625 Val accuracy: 0.9065\n",
      "epoch: 80 loss: 0.31898707 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9088\n",
      "epoch: 90 loss: 0.40013003 Training (batch) accuracy: 0.890625 Val accuracy: 0.9097\n",
      "epoch: 100 loss: 0.3063572 Training (batch) accuracy: 0.921875 Val accuracy: 0.9112\n",
      "epoch: 110 loss: 0.4346578 Training (batch) accuracy: 0.87890625 Val accuracy: 0.912\n",
      "epoch: 120 loss: 0.2899265 Training (batch) accuracy: 0.921875 Val accuracy: 0.9123\n",
      "epoch: 130 loss: 0.30267996 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9127\n",
      "epoch: 140 loss: 0.45397153 Training (batch) accuracy: 0.8828125 Val accuracy: 0.9136\n",
      "epoch: 150 loss: 0.3233696 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9144\n",
      "epoch: 160 loss: 0.26335225 Training (batch) accuracy: 0.9375 Val accuracy: 0.9148\n",
      "epoch: 170 loss: 0.28234234 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9153\n",
      "epoch: 180 loss: 0.27774152 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9159\n",
      "epoch: 190 loss: 0.33653286 Training (batch) accuracy: 0.9140625 Val accuracy: 0.9167\n",
      "epoch: 200 loss: 0.31447324 Training (batch) accuracy: 0.921875 Val accuracy: 0.9165\n",
      "epoch: 210 loss: 0.34231436 Training (batch) accuracy: 0.90625 Val accuracy: 0.9173\n",
      "epoch: 220 loss: 0.20403123 Training (batch) accuracy: 0.953125 Val accuracy: 0.9177\n",
      "epoch: 230 loss: 0.30078685 Training (batch) accuracy: 0.921875 Val accuracy: 0.9181\n",
      "epoch: 240 loss: 0.32632884 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9184\n",
      "epoch: 250 loss: 0.30679867 Training (batch) accuracy: 0.921875 Val accuracy: 0.9187\n",
      "epoch: 260 loss: 0.3638716 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9188\n",
      "epoch: 270 loss: 0.3708092 Training (batch) accuracy: 0.90234375 Val accuracy: 0.9191\n",
      "epoch: 280 loss: 0.2939977 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9194\n",
      "epoch: 290 loss: 0.27971202 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9195\n",
      "epoch: 300 loss: 0.37049687 Training (batch) accuracy: 0.88671875 Val accuracy: 0.9198\n",
      "epoch: 310 loss: 0.24570389 Training (batch) accuracy: 0.90625 Val accuracy: 0.9205\n",
      "epoch: 320 loss: 0.31973684 Training (batch) accuracy: 0.921875 Val accuracy: 0.9206\n",
      "epoch: 330 loss: 0.24057984 Training (batch) accuracy: 0.9375 Val accuracy: 0.9203\n",
      "epoch: 340 loss: 0.3434055 Training (batch) accuracy: 0.90625 Val accuracy: 0.9205\n",
      "epoch: 350 loss: 0.28793663 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9207\n",
      "epoch: 360 loss: 0.2817707 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9209\n",
      "epoch: 370 loss: 0.29599345 Training (batch) accuracy: 0.9140625 Val accuracy: 0.9209\n",
      "epoch: 380 loss: 0.2562722 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9209\n",
      "epoch: 390 loss: 0.33241856 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9209\n",
      "epoch: 400 loss: 0.33225802 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9206\n",
      "epoch: 410 loss: 0.34012663 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9208\n",
      "epoch: 420 loss: 0.27582288 Training (batch) accuracy: 0.921875 Val accuracy: 0.921\n",
      "epoch: 430 loss: 0.32569724 Training (batch) accuracy: 0.8984375 Val accuracy: 0.9211\n",
      "epoch: 440 loss: 0.29091343 Training (batch) accuracy: 0.90625 Val accuracy: 0.9214\n",
      "epoch: 450 loss: 0.22975102 Training (batch) accuracy: 0.921875 Val accuracy: 0.9212\n",
      "epoch: 460 loss: 0.2854711 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9211\n",
      "epoch: 470 loss: 0.3430462 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9213\n",
      "epoch: 480 loss: 0.35171434 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9212\n",
      "epoch: 490 loss: 0.18799299 Training (batch) accuracy: 0.9609375 Val accuracy: 0.9213\n",
      "epoch: 500 loss: 0.36290327 Training (batch) accuracy: 0.8984375 Val accuracy: 0.9211\n",
      "epoch: 510 loss: 0.292654 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9211\n",
      "epoch: 520 loss: 0.33548534 Training (batch) accuracy: 0.90625 Val accuracy: 0.9211\n",
      "epoch: 530 loss: 0.28358507 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9214\n",
      "epoch: 540 loss: 0.2955041 Training (batch) accuracy: 0.9140625 Val accuracy: 0.9212\n",
      "epoch: 550 loss: 0.36134654 Training (batch) accuracy: 0.89453125 Val accuracy: 0.9213\n",
      "epoch: 560 loss: 0.25700843 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9213\n",
      "epoch: 570 loss: 0.26730677 Training (batch) accuracy: 0.921875 Val accuracy: 0.9211\n",
      "epoch: 580 loss: 0.20566835 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9214\n",
      "epoch: 590 loss: 0.1927587 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9215\n",
      "epoch: 600 loss: 0.30195656 Training (batch) accuracy: 0.9375 Val accuracy: 0.9215\n",
      "epoch: 610 loss: 0.2448473 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9215\n",
      "epoch: 620 loss: 0.22497025 Training (batch) accuracy: 0.9375 Val accuracy: 0.9218\n",
      "epoch: 630 loss: 0.33070308 Training (batch) accuracy: 0.9140625 Val accuracy: 0.9219\n",
      "epoch: 640 loss: 0.2732913 Training (batch) accuracy: 0.9296875 Val accuracy: 0.922\n",
      "epoch: 650 loss: 0.25428402 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9221\n",
      "epoch: 660 loss: 0.36465004 Training (batch) accuracy: 0.921875 Val accuracy: 0.9224\n",
      "epoch: 670 loss: 0.24670187 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9226\n",
      "epoch: 680 loss: 0.2686825 Training (batch) accuracy: 0.9375 Val accuracy: 0.9225\n",
      "epoch: 690 loss: 0.2626194 Training (batch) accuracy: 0.94140625 Val accuracy: 0.9225\n",
      "epoch: 700 loss: 0.26261747 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9225\n",
      "epoch: 710 loss: 0.25947392 Training (batch) accuracy: 0.921875 Val accuracy: 0.9228\n",
      "epoch: 720 loss: 0.23223545 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9227\n",
      "epoch: 730 loss: 0.32664692 Training (batch) accuracy: 0.921875 Val accuracy: 0.9229\n",
      "epoch: 740 loss: 0.24494421 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9229\n",
      "epoch: 750 loss: 0.37814027 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9227\n",
      "epoch: 760 loss: 0.23328076 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9226\n",
      "epoch: 770 loss: 0.24023943 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9228\n",
      "epoch: 780 loss: 0.2975859 Training (batch) accuracy: 0.90625 Val accuracy: 0.9229\n",
      "epoch: 790 loss: 0.31926876 Training (batch) accuracy: 0.921875 Val accuracy: 0.9229\n",
      "epoch: 800 loss: 0.26652637 Training (batch) accuracy: 0.921875 Val accuracy: 0.9229\n",
      "epoch: 810 loss: 0.28233021 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9227\n",
      "epoch: 820 loss: 0.24018237 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9232\n",
      "epoch: 830 loss: 0.27091166 Training (batch) accuracy: 0.91796875 Val accuracy: 0.923\n",
      "epoch: 840 loss: 0.42261553 Training (batch) accuracy: 0.91015625 Val accuracy: 0.923\n",
      "epoch: 850 loss: 0.15413445 Training (batch) accuracy: 0.96875 Val accuracy: 0.923\n",
      "epoch: 860 loss: 0.30528176 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9229\n",
      "epoch: 870 loss: 0.36935994 Training (batch) accuracy: 0.88671875 Val accuracy: 0.9232\n",
      "epoch: 880 loss: 0.18964109 Training (batch) accuracy: 0.94921875 Val accuracy: 0.9231\n",
      "epoch: 890 loss: 0.22809157 Training (batch) accuracy: 0.9375 Val accuracy: 0.9231\n",
      "epoch: 900 loss: 0.20285158 Training (batch) accuracy: 0.94921875 Val accuracy: 0.9232\n",
      "epoch: 910 loss: 0.24881586 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9232\n",
      "epoch: 920 loss: 0.39752406 Training (batch) accuracy: 0.9140625 Val accuracy: 0.9233\n",
      "epoch: 930 loss: 0.26522833 Training (batch) accuracy: 0.92578125 Val accuracy: 0.923\n",
      "epoch: 940 loss: 0.19670372 Training (batch) accuracy: 0.94921875 Val accuracy: 0.9233\n",
      "epoch: 950 loss: 0.27273357 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 960 loss: 0.27997655 Training (batch) accuracy: 0.90625 Val accuracy: 0.9235\n",
      "epoch: 970 loss: 0.27361807 Training (batch) accuracy: 0.9375 Val accuracy: 0.9236\n",
      "epoch: 980 loss: 0.26678193 Training (batch) accuracy: 0.9453125 Val accuracy: 0.9237\n",
      "epoch: 990 loss: 0.29656076 Training (batch) accuracy: 0.8984375 Val accuracy: 0.9239\n",
      "epoch: 1000 loss: 0.2304095 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9237\n",
      "epoch: 1010 loss: 0.2303172 Training (batch) accuracy: 0.921875 Val accuracy: 0.9236\n",
      "epoch: 1020 loss: 0.3135133 Training (batch) accuracy: 0.94921875 Val accuracy: 0.9237\n",
      "epoch: 1030 loss: 0.29263908 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9237\n",
      "epoch: 1040 loss: 0.2622586 Training (batch) accuracy: 0.921875 Val accuracy: 0.9238\n",
      "epoch: 1050 loss: 0.31181186 Training (batch) accuracy: 0.921875 Val accuracy: 0.9238\n",
      "epoch: 1060 loss: 0.2960021 Training (batch) accuracy: 0.921875 Val accuracy: 0.9237\n",
      "epoch: 1070 loss: 0.32274312 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9236\n",
      "epoch: 1080 loss: 0.2910135 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9238\n",
      "epoch: 1090 loss: 0.23256005 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9238\n",
      "epoch: 1100 loss: 0.26314136 Training (batch) accuracy: 0.9375 Val accuracy: 0.9236\n",
      "epoch: 1110 loss: 0.28031653 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9236\n",
      "epoch: 1120 loss: 0.2725261 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9237\n",
      "epoch: 1130 loss: 0.3094293 Training (batch) accuracy: 0.90625 Val accuracy: 0.9235\n",
      "epoch: 1140 loss: 0.2375159 Training (batch) accuracy: 0.921875 Val accuracy: 0.9237\n",
      "epoch: 1150 loss: 0.26051083 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9234\n",
      "epoch: 1160 loss: 0.27831194 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9234\n",
      "epoch: 1170 loss: 0.23118737 Training (batch) accuracy: 0.9453125 Val accuracy: 0.9234\n",
      "epoch: 1180 loss: 0.23832889 Training (batch) accuracy: 0.921875 Val accuracy: 0.9235\n",
      "epoch: 1190 loss: 0.2892844 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9236\n",
      "epoch: 1200 loss: 0.2705735 Training (batch) accuracy: 0.9140625 Val accuracy: 0.9234\n",
      "epoch: 1210 loss: 0.20584138 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9235\n",
      "epoch: 1220 loss: 0.2899583 Training (batch) accuracy: 0.91796875 Val accuracy: 0.9234\n",
      "epoch: 1230 loss: 0.3110005 Training (batch) accuracy: 0.90234375 Val accuracy: 0.9235\n",
      "epoch: 1240 loss: 0.17941126 Training (batch) accuracy: 0.96484375 Val accuracy: 0.9234\n",
      "epoch: 1250 loss: 0.21992475 Training (batch) accuracy: 0.95703125 Val accuracy: 0.9235\n",
      "epoch: 1260 loss: 0.2462479 Training (batch) accuracy: 0.9375 Val accuracy: 0.9234\n",
      "epoch: 1270 loss: 0.27259105 Training (batch) accuracy: 0.921875 Val accuracy: 0.9234\n",
      "epoch: 1280 loss: 0.28447565 Training (batch) accuracy: 0.9453125 Val accuracy: 0.9235\n",
      "epoch: 1290 loss: 0.32651746 Training (batch) accuracy: 0.8984375 Val accuracy: 0.9234\n",
      "epoch: 1300 loss: 0.20943287 Training (batch) accuracy: 0.94140625 Val accuracy: 0.9234\n",
      "epoch: 1310 loss: 0.26990563 Training (batch) accuracy: 0.9140625 Val accuracy: 0.9233\n",
      "epoch: 1320 loss: 0.20938386 Training (batch) accuracy: 0.9375 Val accuracy: 0.9233\n",
      "epoch: 1330 loss: 0.2777412 Training (batch) accuracy: 0.9140625 Val accuracy: 0.9233\n",
      "epoch: 1340 loss: 0.23721935 Training (batch) accuracy: 0.9375 Val accuracy: 0.9233\n",
      "epoch: 1350 loss: 0.269032 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9233\n",
      "epoch: 1360 loss: 0.27120295 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9234\n",
      "epoch: 1370 loss: 0.21574922 Training (batch) accuracy: 0.94140625 Val accuracy: 0.9235\n",
      "epoch: 1380 loss: 0.29651403 Training (batch) accuracy: 0.921875 Val accuracy: 0.9236\n",
      "epoch: 1390 loss: 0.26520666 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9235\n",
      "epoch: 1400 loss: 0.20509797 Training (batch) accuracy: 0.9453125 Val accuracy: 0.9235\n",
      "epoch: 1410 loss: 0.19200075 Training (batch) accuracy: 0.9375 Val accuracy: 0.9232\n",
      "epoch: 1420 loss: 0.2708146 Training (batch) accuracy: 0.9140625 Val accuracy: 0.9234\n",
      "epoch: 1430 loss: 0.24614634 Training (batch) accuracy: 0.9453125 Val accuracy: 0.9234\n",
      "epoch: 1440 loss: 0.17502971 Training (batch) accuracy: 0.953125 Val accuracy: 0.9235\n",
      "epoch: 1450 loss: 0.30896685 Training (batch) accuracy: 0.8984375 Val accuracy: 0.9234\n",
      "epoch: 1460 loss: 0.21581477 Training (batch) accuracy: 0.94921875 Val accuracy: 0.9234\n",
      "epoch: 1470 loss: 0.23165326 Training (batch) accuracy: 0.94140625 Val accuracy: 0.9233\n",
      "epoch: 1480 loss: 0.21894965 Training (batch) accuracy: 0.9453125 Val accuracy: 0.9234\n",
      "epoch: 1490 loss: 0.23829846 Training (batch) accuracy: 0.921875 Val accuracy: 0.9236\n",
      "epoch: 1500 loss: 0.30438602 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9236\n",
      "epoch: 1510 loss: 0.2414194 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9233\n",
      "epoch: 1520 loss: 0.21386069 Training (batch) accuracy: 0.94140625 Val accuracy: 0.9236\n",
      "epoch: 1530 loss: 0.21721762 Training (batch) accuracy: 0.9453125 Val accuracy: 0.9237\n",
      "epoch: 1540 loss: 0.23145899 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9236\n",
      "epoch: 1550 loss: 0.23386383 Training (batch) accuracy: 0.95703125 Val accuracy: 0.9237\n",
      "epoch: 1560 loss: 0.27081138 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9237\n",
      "epoch: 1570 loss: 0.2966439 Training (batch) accuracy: 0.89453125 Val accuracy: 0.924\n",
      "epoch: 1580 loss: 0.19344325 Training (batch) accuracy: 0.94921875 Val accuracy: 0.9238\n",
      "epoch: 1590 loss: 0.38725984 Training (batch) accuracy: 0.90234375 Val accuracy: 0.924\n",
      "epoch: 1600 loss: 0.23818351 Training (batch) accuracy: 0.9375 Val accuracy: 0.9241\n",
      "epoch: 1610 loss: 0.33931655 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9241\n",
      "epoch: 1620 loss: 0.26558426 Training (batch) accuracy: 0.91015625 Val accuracy: 0.924\n",
      "epoch: 1630 loss: 0.25091362 Training (batch) accuracy: 0.91796875 Val accuracy: 0.924\n",
      "epoch: 1640 loss: 0.30413312 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9239\n",
      "epoch: 1650 loss: 0.17961822 Training (batch) accuracy: 0.9453125 Val accuracy: 0.9242\n",
      "epoch: 1660 loss: 0.30504978 Training (batch) accuracy: 0.90625 Val accuracy: 0.9243\n",
      "epoch: 1670 loss: 0.12938024 Training (batch) accuracy: 0.96484375 Val accuracy: 0.9242\n",
      "epoch: 1680 loss: 0.2884465 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9242\n",
      "epoch: 1690 loss: 0.25735316 Training (batch) accuracy: 0.92578125 Val accuracy: 0.924\n",
      "epoch: 1700 loss: 0.20113713 Training (batch) accuracy: 0.953125 Val accuracy: 0.9243\n",
      "epoch: 1710 loss: 0.271783 Training (batch) accuracy: 0.91015625 Val accuracy: 0.9244\n",
      "epoch: 1720 loss: 0.20856312 Training (batch) accuracy: 0.953125 Val accuracy: 0.9244\n",
      "epoch: 1730 loss: 0.30483067 Training (batch) accuracy: 0.921875 Val accuracy: 0.9243\n",
      "epoch: 1740 loss: 0.24308415 Training (batch) accuracy: 0.9453125 Val accuracy: 0.9242\n",
      "epoch: 1750 loss: 0.2644543 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9245\n",
      "epoch: 1760 loss: 0.29575926 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9244\n",
      "epoch: 1770 loss: 0.22705692 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9248\n",
      "epoch: 1780 loss: 0.266267 Training (batch) accuracy: 0.94140625 Val accuracy: 0.9247\n",
      "epoch: 1790 loss: 0.22312787 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9247\n",
      "epoch: 1800 loss: 0.25340638 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9246\n",
      "epoch: 1810 loss: 0.24289027 Training (batch) accuracy: 0.9140625 Val accuracy: 0.9249\n",
      "epoch: 1820 loss: 0.28588864 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9249\n",
      "epoch: 1830 loss: 0.20356035 Training (batch) accuracy: 0.94921875 Val accuracy: 0.9248\n",
      "epoch: 1840 loss: 0.19247529 Training (batch) accuracy: 0.94140625 Val accuracy: 0.9249\n",
      "epoch: 1850 loss: 0.30190504 Training (batch) accuracy: 0.90625 Val accuracy: 0.9247\n",
      "epoch: 1860 loss: 0.22673708 Training (batch) accuracy: 0.94921875 Val accuracy: 0.925\n",
      "epoch: 1870 loss: 0.24135077 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9248\n",
      "epoch: 1880 loss: 0.22243288 Training (batch) accuracy: 0.9375 Val accuracy: 0.925\n",
      "epoch: 1890 loss: 0.22371475 Training (batch) accuracy: 0.921875 Val accuracy: 0.9248\n",
      "epoch: 1900 loss: 0.23900858 Training (batch) accuracy: 0.921875 Val accuracy: 0.9251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1910 loss: 0.24972343 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9249\n",
      "epoch: 1920 loss: 0.28432736 Training (batch) accuracy: 0.9296875 Val accuracy: 0.9249\n",
      "epoch: 1930 loss: 0.23483682 Training (batch) accuracy: 0.9296875 Val accuracy: 0.925\n",
      "epoch: 1940 loss: 0.31743768 Training (batch) accuracy: 0.9375 Val accuracy: 0.9249\n",
      "epoch: 1950 loss: 0.30538252 Training (batch) accuracy: 0.93359375 Val accuracy: 0.9249\n",
      "epoch: 1960 loss: 0.28561026 Training (batch) accuracy: 0.92578125 Val accuracy: 0.9249\n",
      "epoch: 1970 loss: 0.26672268 Training (batch) accuracy: 0.94921875 Val accuracy: 0.925\n",
      "epoch: 1980 loss: 0.260058 Training (batch) accuracy: 0.9140625 Val accuracy: 0.9247\n",
      "epoch: 1990 loss: 0.2039333 Training (batch) accuracy: 0.94921875 Val accuracy: 0.925\n"
     ]
    }
   ],
   "source": [
    "# Execute\n",
    "n_epochs = 2000\n",
    "batch_size = 256\n",
    "# number of training samples is critical for training your deep neural network\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "#         for iteration in range (train_X.shape[0]// batch_size):\n",
    "        for X_batch, y_batch in shuffle_batch (train_X, train_y, batch_size):\n",
    "            _, loss_val = sess.run([training_op, l], feed_dict={X: X_batch, y: y_batch })\n",
    "        if epoch % 10 == 0:    \n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_val = accuracy.eval(feed_dict={X: test_X, y: test_y})\n",
    "            print(\"epoch:\", epoch, \"loss:\",loss_val , \"Training (batch) accuracy:\", acc_train, \"Val accuracy:\", acc_val)\n",
    "            \n",
    "#     save_path= saver.save(sess, \"myFirstModel.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring training on larger datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks:\n",
    "    [0] Use MNIST data to train your network (play with different network architectures!!!)\n",
    "    [1] Split the data into train-val set and retrain your model also computing validation accuracy\n",
    "    [2] Make deeper network to achieve higher performance of your training model\n",
    "    [3] Visualize your loss and accuracy on tensorboard\n",
    "    [4] Add droppout layer and retrain your model. Comment on the difference that you have seen in the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help for loading MNIST data\n",
    "import numpy as np\n",
    "data = np.load('datasets/MNIST_data/mnist.npz')\n",
    "x_train= data['x_train']\n",
    "y_train = data['y_train']\n",
    "x_test = data['x_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# Note if this does not work then use below\n",
    "# mnist = tf.keras.datasets.mnist\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alitorch",
   "language": "python",
   "name": "alitorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
